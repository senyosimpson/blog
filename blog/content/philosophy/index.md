---
hidden: true
---

An enumeration of my core philosophy w.r.t my career. This is more for documentation purposes than anything but I think some people may find it useful hence I am sharing it openly.

## Stay lean

*Staying lean means purposefully staying focused on a small set of topics and extracting maximum value from them.*

As with most people who are passionate about the work they do, I have about 1000 side projects and ideas I want to spend time on. There are too many books I want to read, podcasts I want to listen to, talks I want to watch. There is an endless amount of content to consume and obviously, not enough time to do so. They are also all *equally* interesting. Service meshes, stream processing, distributed SQL, functional programming, Rust, k8s. It never ends. The billion dollar insight is that they are *not all equally useful* to me at a given point in time. Staying lean means purposefully staying focused on a small set of topics and extracting maximum value from them. This has enabled me to learn at a more rapid pace and acquire a much deeper knowledge of the topics of interest.

## Bayesian optimize

*Choose topics that are highly promising and have the potential to maximize learning.*

Staying lean is unnatural. With so many compelling alternatives, it is difficult to choose only one or two. The most difficult question is, "what do I do next?". In this [blog post](https://distill.pub/2020/bayesian-optimization/), they explain how Bayesian optimization is useful for hyperparameter tuning of machine learning models. For those that don't know, hyperparameter tuning can be thought of as the process to discover the settings that maximize performance of a machine learning model. Bayesian optimization aims to find the optimal settings in the least amount of time possible. It achieves this by estimating the set of optimal settings given the information it has at present. As it makes more estimates, the amount of information available to it increases, allowing for better informed estimates. The key is that estimates are done in a smart fashion by focusing on regions that are **highly promising**.

What can I take away from this? I should Bayesian optimize my life... such a nerd ðŸ¤“. To stay lean, I should choose topics that will maximise my learning. These can be thought of as topics that are highly promising and have the potential to maximize learning. Again, this is non-trivial since most topics will teach you plenty. How do you even define maximum learning? To make this easier, topics should be contextualised to my current position. That is to say, the topics that will maximise my learning are the ones most relevant at the given point in time. A simple example: there is no need to learn how to operate k8s if I don't use it at work, am not contributing to it and it is not going to be used in my foreseeable future. So, to stay lean, apply the spirit of Bayesian optimization. This is done by identifying what is important at present, ranking them and choosing the top one or two topics.

## Build for tomorrow

*Do things today to ensure success for your future.*

I am strong believer in "making things happen". If you really want something, you will be inclined to put the steps in place to make it a reality. This is the premise for "build for tomorrow". I must ensure that I have a north star and put the necessary steps in place to achieve it. Alongside this, I should strive to take opportunities that will fast track me to achieving it.


## Fundamentals first

*Focus on learning the core fundamentals.*

Tech is a huge, fast-evolving field. It is literally impossible to keep up. However, this rapid evolution often happens at the "application layer". A good case in point: artificial neural networks have been propelled into prominence due to their outstanding performance in a wide variety of tasks - especially those related to computer vision and natural language processing. It is fair to say that the application of neural networks is one of the most important technological outcomes of the 2010s. Interestingly enough, the foundational work on neural networks started in 1943. Convolutional neural networks were first formalised in the 1980s. These *core* fundamentals are still the same now as they were then. If you were to focus on the applications of convolutional neural networks, you would drown in new papers every single week and continuously have to change your architectures, training regimes, datasets and so on to keep up. A focus on the fundamentals will ensure you have a small, well-refined and good personal "standard library". This will improve your ability to discern what is relevant and important to your use case and allow you to learn higher level concepts much more rapidly.

I do not strive to keep up with all the new and exciting technologies - it would just swallow all of my time. A better use of time is to understand the core fundamentals and build a hierarchy of knowledge from that foundation.

## Sink and swim

*Dive headfirst into the deep end. Foraying out of your comfort zone presents you with the opportunity to grow significantly.*

I learn best in the deep end. Not knowing much about the problem I'm solving really helps me to build a 360Â° view of it. It allows for more exploration and significantly deepens understanding of the problem itself and the domain as a whole. Also, when the lightbulb finally switches on, it's an amazing moment ðŸ’¡. I am a believer in diving into the deep end and leaving your comfort zone. It the *fastest* way to grow, with the side effect of ensuring you actually do grow. Life begins at the end of your comfort zone.